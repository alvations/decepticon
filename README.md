# Decepticon


<table>
  <tbody>
    <tr>
      <th align="left">Topic</th>
      <th align="left">Readings</th>
    </tr>
    <tr>
      <td align="left">Neural Language Model (Pre-2015)</td>
      <td align="left">
      <ul>
        <li>
          <a href="https://crl.ucsd.edu/~elman/Papers/fsit.pdf">Elman (1990)</a>
        </li>
        <li>
          <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et al. (2003)</a>
        </li>
        <li>
          <a href="https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf">Collobert and Weston (2008)</a>
        </li>
        <li>
          <a href="https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Mikolov et al. (2010)</a>
        </li>
        <li>
          <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Collobert et al. (2011)</a>
        </li>
        <li>
          <a href="https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf">Sutskever et al. (2011)</a>
        </li>
        <li>
          <a href="https://nlg.isi.edu/software/nplm/vaswani-emnlp13.pdf">Vaswani et al. (2013)</a>
        </li>
      </ul>
      </td>
    </tr>
    <tr>
      <td align="left">RNN Variants</td>
      <td align="left">
      <ul>
        <li><a href="https://www2.informatik.uni-hamburg.de/wtm/ps/Alpay_ICANN_2016.pdf">RNN Timescales variants (Alpay et al. 2016)</a></li>
        <li><a href="https://arxiv.org/pdf/1503.04069.pdf">LSTM Gates variants (Greff et al. 2015)</a></li>
        <li><a href="https://arxiv.org/pdf/1603.08983.pdf">Active Computation Time (Graves, 2016)</a></li>
      </ul>
      </td>
    </tr>
    <tr>
      <td align="left">Sequence to Sequence (Pre-Transformer)</td>
      <td align="left">
      <ul>
        <li><a href="https://www.aclweb.org/anthology/D13-1176.pdf">Kalchbrenner and Blunsom (2013)</a></li>
        <li><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al. (2014)</a></li>
        <li><a href="https://arxiv.org/abs/1409.0473">Bahdanau et al. (2015)</a></li>
        <li><a href="https://research.google/pubs/pub45610/">GNMT (Wu et al. 2016)</a></li>
      </ul>
      </td>
    </tr>
    <tr>
      <td align="left">Transformers</td>
      <td align="left">
      <ul>
        <li><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need">Vaswani et al. (2017)</a></li>
      </ul>
      </td>
    </tr>  
    <tr>
      <td align="left">Training Tricks</td>
      <td align="left">
      <ul>
        <li><a href="https://www.aclweb.org/anthology/E17-2025.pdf">Tied Embeddings (Press and Wolf, 2017)</a></li>
        <li><a href="https://arxiv.org/abs/1512.03385">Residual Connection (He et al. 2015)</a></li>
        <li><a href="https://arxiv.org/abs/1608.06993">Dense Connection (Huang et al. 2017)</a></li>
      </ul>
      </td>
    </tr>
    <tr>
      <td align="left">Hybrid Recurrent Transformers</td>
      <td align="left">
      <ul>
        <li><a href="https://www.aclweb.org/anthology/P18-1008.pdf">RNMT+ (Chen et al. 2018)</a></li>
        <li><a href="https://arxiv.org/abs/1807.03819">Universal Transformer (Dehghani et al. 2019)</a></li>
      </ul>
      </td>
    </tr>
    <tr>
      <td align="left">Non-Autoregressive</td>
      <td align="left">
      <ul>
        <li><a href="https://arxiv.org/abs/1711.02281">Non-Autoregressive NMT (Gu et al. 2018)</a></li>
        <li><a href="https://papers.nips.cc/paper/9297-levenshtein-transformer.pdf">Levenshtein Transformer (Gu et al. 2019)</a></li>
      </ul>
      </td>
    </tr>
  </tbody>
</table>
  
